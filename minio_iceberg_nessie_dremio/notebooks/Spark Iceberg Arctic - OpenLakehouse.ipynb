{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53e0cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the AWS env variables if you are using AWS Auth:\n",
    "%env AWS_REGION=REGION\n",
    "%env AWS_ACCESS_KEY_ID=KEY\n",
    "%env AWS_SECRET_ACCESS_KEY=SECRET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9771f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/docker/.local/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/docker/.ivy2/cache\n",
      "The jars for the packages stored in: /home/docker/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.3_2.12 added as a dependency\n",
      "org.projectnessie#nessie-spark-extensions-3.3_2.12 added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1f92b084-5338-43a3-b8af-b4b4fa42f79b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.3_2.12;1.0.0 in central\n",
      "\tfound org.projectnessie#nessie-spark-extensions-3.3_2.12;0.44.0 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.17 in central\n",
      "\tfound org.projectnessie#nessie-spark-extensions-grammar;0.44.0 in central\n",
      "\tfound org.projectnessie#nessie-spark-antlr-runtime;0.44.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.11.1 in central\n",
      "\tfound org.projectnessie#nessie-spark-extensions-base_2.12;0.44.0 in central\n",
      "\tfound org.projectnessie#nessie-client;0.30.0 in central\n",
      "\tfound org.projectnessie#nessie-model;0.30.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.17.178 in central\n",
      "\tfound software.amazon.eventstream#eventstream;1.0.1 in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.17.178 in central\n",
      "\tfound software.amazon.awssdk#utils;2.17.178 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.17.178 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.17.178 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.17.178 in central\n",
      ":: resolution report :: resolve 11361ms :: artifacts dl 531ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.11.1 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.3_2.12;1.0.0 from central in [default]\n",
      "\torg.projectnessie#nessie-client;0.30.0 from central in [default]\n",
      "\torg.projectnessie#nessie-model;0.30.0 from central in [default]\n",
      "\torg.projectnessie#nessie-spark-antlr-runtime;0.44.0 from central in [default]\n",
      "\torg.projectnessie#nessie-spark-extensions-3.3_2.12;0.44.0 from central in [default]\n",
      "\torg.projectnessie#nessie-spark-extensions-base_2.12;0.44.0 from central in [default]\n",
      "\torg.projectnessie#nessie-spark-extensions-grammar;0.44.0 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.17 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.eventstream#eventstream;1.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   19  |   0   |   0   |   0   ||   19  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1f92b084-5338-43a3-b8af-b4b4fa42f79b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 19 already retrieved (0kB/256ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 18:49:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/25 18:50:03 ERROR Inbox: Ignoring error\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/11/25 18:50:04 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\t... 3 more\n",
      "Spark Running\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import findspark\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "conf = SparkConf()\n",
    "# we need iceberg libraries and the nessie sql extensions\n",
    "conf.set(\n",
    "    \"spark.jars.packages\",\n",
    "    f\"org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.0.0,org.projectnessie:nessie-spark-extensions-3.3_2.12:0.44.0,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178\",\n",
    ")\n",
    "# ensure python <-> java interactions are w/ pyarrow\n",
    "conf.set(\"spark.sql.execution.pyarrow.enabled\", \"true\")\n",
    "\n",
    "# Config to change the IO implementation of target catalog; write to object store\n",
    "conf.set(\"spark.sql.catalog.arctic.io-impl\",\"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "\n",
    "# create catalog named arctic as an iceberg catalog\n",
    "conf.set(\"spark.sql.catalog.arctic\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "\n",
    "# tell the catalog that its a Nessie catalog\n",
    "conf.set(\"spark.sql.catalog.arctic.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "\n",
    "# set the location for the catalog to store data. Spark writes to this directory\n",
    "conf.set(\"spark.sql.catalog.arctic.warehouse\", \"s3://bucket/\")\n",
    "\n",
    "# set the location of the Arctic/Nessie server.\n",
    "conf.set(\"spark.sql.catalog.arctic.uri\", \"http://localhost:19120/api/v1\")\n",
    "\n",
    "# default branch for Arctic catalog to work on\n",
    "conf.set(\"spark.sql.catalog.arctic.ref\", \"main\")\n",
    "\n",
    "# Authentication mechanism. Here, we use AWS with BEARER\n",
    "conf.set(\"spark.sql.catalog.arctic.authentication.type\", \"BEARER\")\n",
    "conf.set(\"spark.sql.catalog.arctic.authentication.token\", \"ACCESS TOKEN\")\n",
    "\n",
    "# enable the extensions for both Nessie and Iceberg\n",
    "conf.set(\n",
    "    \"spark.sql.extensions\",\n",
    "    \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\",\n",
    ")\n",
    "\n",
    "# finally, start up the Spark server\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "print(\"Spark Running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdec1b0e",
   "metadata": {},
   "source": [
    "# Create a new branch called work in the catalog for our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4faf66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>refType</th>\n",
       "      <th>name</th>\n",
       "      <th>hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Branch</td>\n",
       "      <td>work</td>\n",
       "      <td>d1d5da6c513e6bc2b615a1de89c7a30b587302a84bb60b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  refType  name                                               hash\n",
       "0  Branch  work  d1d5da6c513e6bc2b615a1de89c7a30b587302a84bb60b..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE BRANCH work IN arctic\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61848f26",
   "metadata": {},
   "source": [
    "# Use the newly created branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d1db96a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[refType: string, name: string, hash: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE REFERENCE work IN arctic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab836c90",
   "metadata": {},
   "source": [
    "# Create a new Iceberg table 'sales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14bdb9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"CREATE TABLE IF NOT EXISTS arctic.salesdip.sales\n",
    "            (id STRING, name STRING, product STRING, price STRING, date STRING) USING iceberg\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce82745",
   "metadata": {},
   "source": [
    "# INSERT new data from a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0408dd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"CREATE OR REPLACE TEMPORARY VIEW salesview USING csv\n",
    "            OPTIONS (path \"salesdata.csv\", header true)\"\"\"\n",
    ")\n",
    "spark.sql(\"INSERT INTO arctic.salesdip.sales SELECT * FROM salesview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3c1564",
   "metadata": {},
   "source": [
    "# Read the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74c0c211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>product</th>\n",
       "      <th>price</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Selinda Rheaume</td>\n",
       "      <td>Wine - Prosecco Valdobiaddene</td>\n",
       "      <td>10.31</td>\n",
       "      <td>10/20/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Wynnie Gozzard</td>\n",
       "      <td>Wine - Red, Wolf Blass, Yellow</td>\n",
       "      <td>20.05</td>\n",
       "      <td>10/20/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Patten Whitter</td>\n",
       "      <td>Crackers - Soda / Saltins</td>\n",
       "      <td>39.75</td>\n",
       "      <td>10/20/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Hulda Eslie</td>\n",
       "      <td>Roe - White Fish</td>\n",
       "      <td>100</td>\n",
       "      <td>10/20/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Chrystal Haggie</td>\n",
       "      <td>Wine - Delicato Merlot</td>\n",
       "      <td>35.82</td>\n",
       "      <td>10/20/2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id             name                         product  price        date\n",
       "0  1  Selinda Rheaume   Wine - Prosecco Valdobiaddene  10.31  10/20/2022\n",
       "1  2   Wynnie Gozzard  Wine - Red, Wolf Blass, Yellow  20.05  10/20/2022\n",
       "2  3   Patten Whitter       Crackers - Soda / Saltins  39.75  10/20/2022\n",
       "3  4      Hulda Eslie                Roe - White Fish    100  10/20/2022\n",
       "4  5  Chrystal Haggie          Wine - Delicato Merlot  35.82  10/20/2022"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM arctic.salesdip.sales LIMIT 5\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac559a14",
   "metadata": {},
   "source": [
    "# DML - Update the table with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "383565b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"UPDATE arctic.salesdip.sales SET price = 100 WHERE id = 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177c57ff",
   "metadata": {},
   "source": [
    "# Iceberg default Metadata Tables: we query the 'files', 'history', 'snapshots' table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "395ba3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>file_path</th>\n",
       "      <th>file_format</th>\n",
       "      <th>spec_id</th>\n",
       "      <th>record_count</th>\n",
       "      <th>file_size_in_bytes</th>\n",
       "      <th>column_sizes</th>\n",
       "      <th>value_counts</th>\n",
       "      <th>null_value_counts</th>\n",
       "      <th>nan_value_counts</th>\n",
       "      <th>lower_bounds</th>\n",
       "      <th>upper_bounds</th>\n",
       "      <th>key_metadata</th>\n",
       "      <th>split_offsets</th>\n",
       "      <th>equality_ids</th>\n",
       "      <th>sort_order_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>s3://dremio-58d63711-a193-4ad2-a226-52dfcb1291...</td>\n",
       "      <td>PARQUET</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>14775</td>\n",
       "      <td>{1: 949, 2: 4912, 3: 5979, 4: 1584, 5: 101}</td>\n",
       "      <td>{1: 500, 2: 500, 3: 500, 4: 500, 5: 500}</td>\n",
       "      <td>{1: 0, 2: 0, 3: 0, 4: 0, 5: 0}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{1: [49], 2: [65, 98, 98, 111, 116, 32, 72, 10...</td>\n",
       "      <td>{1: [57, 57], 2: [90, 101, 98, 117, 108, 111, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>[4]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   content                                          file_path file_format  \\\n",
       "0        0  s3://dremio-58d63711-a193-4ad2-a226-52dfcb1291...     PARQUET   \n",
       "\n",
       "   spec_id  record_count  file_size_in_bytes  \\\n",
       "0        0           500               14775   \n",
       "\n",
       "                                  column_sizes  \\\n",
       "0  {1: 949, 2: 4912, 3: 5979, 4: 1584, 5: 101}   \n",
       "\n",
       "                               value_counts               null_value_counts  \\\n",
       "0  {1: 500, 2: 500, 3: 500, 4: 500, 5: 500}  {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}   \n",
       "\n",
       "  nan_value_counts                                       lower_bounds  \\\n",
       "0               {}  {1: [49], 2: [65, 98, 98, 111, 116, 32, 72, 10...   \n",
       "\n",
       "                                        upper_bounds key_metadata  \\\n",
       "0  {1: [57, 57], 2: [90, 101, 98, 117, 108, 111, ...         None   \n",
       "\n",
       "  split_offsets equality_ids  sort_order_id  \n",
       "0           [4]         None              0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM arctic.salesdip.sales.files\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b83ddee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/docker/.local/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>made_current_at</th>\n",
       "      <th>snapshot_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>is_current_ancestor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-11-25 19:00:31.662</td>\n",
       "      <td>6202746828304796124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-25 20:23:15.635</td>\n",
       "      <td>2878892377637294950</td>\n",
       "      <td>6.202747e+18</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-11-25 20:24:46.203</td>\n",
       "      <td>1747027818651198456</td>\n",
       "      <td>2.878892e+18</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          made_current_at          snapshot_id     parent_id  \\\n",
       "0 2022-11-25 19:00:31.662  6202746828304796124           NaN   \n",
       "1 2022-11-25 20:23:15.635  2878892377637294950  6.202747e+18   \n",
       "2 2022-11-25 20:24:46.203  1747027818651198456  2.878892e+18   \n",
       "\n",
       "   is_current_ancestor  \n",
       "0                 True  \n",
       "1                 True  \n",
       "2                 True  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM arctic.salesdip.sales.history\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9aa640ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/docker/.local/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>committed_at</th>\n",
       "      <th>snapshot_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>operation</th>\n",
       "      <th>manifest_list</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-11-25 19:00:31.662</td>\n",
       "      <td>6202746828304796124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>append</td>\n",
       "      <td>s3://dremio-58d63711-a193-4ad2-a226-52dfcb1291...</td>\n",
       "      <td>{'spark.app.id': 'local-1669402184833', 'chang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-25 20:23:15.635</td>\n",
       "      <td>2878892377637294950</td>\n",
       "      <td>6.202747e+18</td>\n",
       "      <td>overwrite</td>\n",
       "      <td>s3://dremio-58d63711-a193-4ad2-a226-52dfcb1291...</td>\n",
       "      <td>{'spark.app.id': 'local-1669402184833', 'chang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-11-25 20:24:46.203</td>\n",
       "      <td>1747027818651198456</td>\n",
       "      <td>2.878892e+18</td>\n",
       "      <td>overwrite</td>\n",
       "      <td>s3://dremio-58d63711-a193-4ad2-a226-52dfcb1291...</td>\n",
       "      <td>{'added-data-files': '1', 'total-equality-dele...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             committed_at          snapshot_id     parent_id  operation  \\\n",
       "0 2022-11-25 19:00:31.662  6202746828304796124           NaN     append   \n",
       "1 2022-11-25 20:23:15.635  2878892377637294950  6.202747e+18  overwrite   \n",
       "2 2022-11-25 20:24:46.203  1747027818651198456  2.878892e+18  overwrite   \n",
       "\n",
       "                                       manifest_list  \\\n",
       "0  s3://dremio-58d63711-a193-4ad2-a226-52dfcb1291...   \n",
       "1  s3://dremio-58d63711-a193-4ad2-a226-52dfcb1291...   \n",
       "2  s3://dremio-58d63711-a193-4ad2-a226-52dfcb1291...   \n",
       "\n",
       "                                             summary  \n",
       "0  {'spark.app.id': 'local-1669402184833', 'chang...  \n",
       "1  {'spark.app.id': 'local-1669402184833', 'chang...  \n",
       "2  {'added-data-files': '1', 'total-equality-dele...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM arctic.salesdip.sales.snapshots\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a0cac4",
   "metadata": {},
   "source": [
    "# Iceberg Time Travel demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f81e846b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>product</th>\n",
       "      <th>price</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Selinda Rheaume</td>\n",
       "      <td>Wine - Prosecco Valdobiaddene</td>\n",
       "      <td>10.31</td>\n",
       "      <td>10/20/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Wynnie Gozzard</td>\n",
       "      <td>Wine - Red, Wolf Blass, Yellow</td>\n",
       "      <td>20.05</td>\n",
       "      <td>10/20/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Patten Whitter</td>\n",
       "      <td>Crackers - Soda / Saltins</td>\n",
       "      <td>39.75</td>\n",
       "      <td>10/20/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Hulda Eslie</td>\n",
       "      <td>Roe - White Fish</td>\n",
       "      <td>37.10</td>\n",
       "      <td>10/20/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Chrystal Haggie</td>\n",
       "      <td>Wine - Delicato Merlot</td>\n",
       "      <td>35.82</td>\n",
       "      <td>10/20/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>496</td>\n",
       "      <td>Winny McGlone</td>\n",
       "      <td>Pasta - Fusili Tri - Coloured</td>\n",
       "      <td>67.78</td>\n",
       "      <td>10/20/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>497</td>\n",
       "      <td>Hadleigh Ellinor</td>\n",
       "      <td>Oats Large Flake</td>\n",
       "      <td>58.19</td>\n",
       "      <td>10/20/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>498</td>\n",
       "      <td>Kimberlee Hancill</td>\n",
       "      <td>Soup - Knorr, Country Bean</td>\n",
       "      <td>81.64</td>\n",
       "      <td>10/20/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>499</td>\n",
       "      <td>Anet Scaife</td>\n",
       "      <td>Soup - French Onion</td>\n",
       "      <td>98.51</td>\n",
       "      <td>10/20/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>500</td>\n",
       "      <td>Bary Reap</td>\n",
       "      <td>Soup - Tomato Mush. Florentine</td>\n",
       "      <td>92.64</td>\n",
       "      <td>10/20/2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id               name                         product  price        date\n",
       "0      1    Selinda Rheaume   Wine - Prosecco Valdobiaddene  10.31  10/20/2022\n",
       "1      2     Wynnie Gozzard  Wine - Red, Wolf Blass, Yellow  20.05  10/20/2022\n",
       "2      3     Patten Whitter       Crackers - Soda / Saltins  39.75  10/20/2022\n",
       "3      4        Hulda Eslie                Roe - White Fish  37.10  10/20/2022\n",
       "4      5    Chrystal Haggie          Wine - Delicato Merlot  35.82  10/20/2022\n",
       "..   ...                ...                             ...    ...         ...\n",
       "495  496      Winny McGlone   Pasta - Fusili Tri - Coloured  67.78  10/20/2022\n",
       "496  497   Hadleigh Ellinor                Oats Large Flake  58.19  10/20/2022\n",
       "497  498  Kimberlee Hancill      Soup - Knorr, Country Bean  81.64  10/20/2022\n",
       "498  499        Anet Scaife             Soup - French Onion  98.51  10/20/2022\n",
       "499  500          Bary Reap  Soup - Tomato Mush. Florentine  92.64  10/20/2022\n",
       "\n",
       "[500 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM arctic.salesdip.sales TIMESTAMP AS OF '2022-11-25 19:00:31.662' \").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc11a302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>product</th>\n",
       "      <th>price</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Selinda Rheaume</td>\n",
       "      <td>Wine - Prosecco Valdobiaddene</td>\n",
       "      <td>10.31</td>\n",
       "      <td>10/20/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Wynnie Gozzard</td>\n",
       "      <td>Wine - Red, Wolf Blass, Yellow</td>\n",
       "      <td>20.05</td>\n",
       "      <td>10/20/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Patten Whitter</td>\n",
       "      <td>Crackers - Soda / Saltins</td>\n",
       "      <td>39.75</td>\n",
       "      <td>10/20/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Hulda Eslie</td>\n",
       "      <td>Roe - White Fish</td>\n",
       "      <td>100</td>\n",
       "      <td>10/20/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Chrystal Haggie</td>\n",
       "      <td>Wine - Delicato Merlot</td>\n",
       "      <td>35.82</td>\n",
       "      <td>10/20/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>496</td>\n",
       "      <td>Winny McGlone</td>\n",
       "      <td>Pasta - Fusili Tri - Coloured</td>\n",
       "      <td>67.78</td>\n",
       "      <td>10/20/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>497</td>\n",
       "      <td>Hadleigh Ellinor</td>\n",
       "      <td>Oats Large Flake</td>\n",
       "      <td>58.19</td>\n",
       "      <td>10/20/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>498</td>\n",
       "      <td>Kimberlee Hancill</td>\n",
       "      <td>Soup - Knorr, Country Bean</td>\n",
       "      <td>81.64</td>\n",
       "      <td>10/20/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>499</td>\n",
       "      <td>Anet Scaife</td>\n",
       "      <td>Soup - French Onion</td>\n",
       "      <td>98.51</td>\n",
       "      <td>10/20/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>500</td>\n",
       "      <td>Bary Reap</td>\n",
       "      <td>Soup - Tomato Mush. Florentine</td>\n",
       "      <td>92.64</td>\n",
       "      <td>10/20/2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id               name                         product  price        date\n",
       "0      1    Selinda Rheaume   Wine - Prosecco Valdobiaddene  10.31  10/20/2022\n",
       "1      2     Wynnie Gozzard  Wine - Red, Wolf Blass, Yellow  20.05  10/20/2022\n",
       "2      3     Patten Whitter       Crackers - Soda / Saltins  39.75  10/20/2022\n",
       "3      4        Hulda Eslie                Roe - White Fish    100  10/20/2022\n",
       "4      5    Chrystal Haggie          Wine - Delicato Merlot  35.82  10/20/2022\n",
       "..   ...                ...                             ...    ...         ...\n",
       "495  496      Winny McGlone   Pasta - Fusili Tri - Coloured  67.78  10/20/2022\n",
       "496  497   Hadleigh Ellinor                Oats Large Flake  58.19  10/20/2022\n",
       "497  498  Kimberlee Hancill      Soup - Knorr, Country Bean  81.64  10/20/2022\n",
       "498  499        Anet Scaife             Soup - French Onion  98.51  10/20/2022\n",
       "499  500          Bary Reap  Soup - Tomato Mush. Florentine  92.64  10/20/2022\n",
       "\n",
       "[500 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM arctic.salesdip.sales TIMESTAMP AS OF '2022-11-25 20:24:46.203' \").toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
